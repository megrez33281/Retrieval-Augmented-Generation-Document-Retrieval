
# 🧭 實驗流程：使用 OR-ShARC 驗證 RAG 系統的匹配能力

---

## 🔹 第 0 步：明確實驗目的

> 驗證你的檢索器（Retriever）能否在大型規則文本集合中，根據問題與情境，正確匹配出相關的規則段落。

重點不在生成答案，也不在多輪對話或邏輯推理。
測量指標為：

* **Recall@k**
* **MRR (Mean Reciprocal Rank)**

---

## 🔹 第 1 步：準備資料集
目前OR-ShARC已經被下載下來，存到Dataset_OR-ShARC資料夾中，包含：
* `id2snippet.json`（所有規則文本，作為知識庫）
* `train.json`, `dev.json`, `test.json`（問題資料）
一般建議使用 `dev.json` 作為驗證集

---

## 🔹 第 2 步：將 OR-ShARC 轉成 RAG 相容格式

### 📘 目標：

生成以下兩個檔案，格式與`evaluate.py` 相同：

| 檔案名稱              | 用途    | 內容結構                     |
| ----------------- | ----- | ------------------------ |
| `chunks.json`     | 知識庫內容 | 所有 snippet 的文字與 ID       |
| `golden_set.json` | 驗證問題集 | 問題（含情境）與對應的正確 snippet ID |

要先檢索並確認這兩個資料的格式

---

### 轉換流程（文字描述）

1. **載入資料**

   * 讀取 `id2snippet.json` 為一個字典 `{id: text}`。
   * 讀取 `dev.json`（或 `train.json`/`test.json`）。

2. **生成知識庫 (`chunks.json`)**

   * 將每條 `id2snippet` 轉為：

     ```python
     {"chunk_id": int(id), "text": text}
     ```
   * 儲存成 `chunks.json`　

3. **生成驗證集 (`golden_set.json`)**

   * 對 `dev.json` 中每個樣本：

     * 將 `question` 與 `scenario` 合併成單一查詢字串：

       ```
       query = question + " " + scenario
       ```
     * 取出其對應的 `gold_snippet_id` 作為 `relevant_chunk_ids`。
     * 建立一筆記錄：

       ```python
       {"question": query, "relevant_chunk_ids": [gold_snippet_id]}
       ```
   * 儲存為 `golden_set.json`。

4. （選擇性）可依需要過濾：
   * 只保留第一輪（初始問題，沒有 history 的樣本）


---

## 第 3 步：執行匹配能力評估

１. 在啟動虛擬環境後執行：

   ```bash
   python evaluate.py
   ```

２. 腳本將會：

   * 使用 `sentence-transformers` 進行向量化；
   * 透過 FAISS 檢索；
   * 針對每個問題找出 top-k chunks；
   * 與 `golden_set.json` 比對計算：

     * Recall@3
     * MRR
   * 輸出詳細結果表與總體平均

---

##  第 4 步：分析結果

### 核心指標

| 指標           | 含義                 |
| ------------ | ------------------ |
| **Recall@3** | 前 3 個檢索結果中是否包含正確規則 |
| **MRR**      | 正確規則在排名中平均靠前程度     |

###  可進一步分析：

1. **模型比較**：

   * `all-MiniLM-L6-v2` vs `all-mpnet-base-v2` vs `E5-large`
   * 看哪個 embedding 模型對規則文字最敏感。
2. **情境增強**：

   * 比較 `question` vs `question + scenario` 檢索效果
3. **seen/unseen 規則**：
   * 根據 `snippet_seen` 分析泛化性能

---

## 🔹 第 5 步（選擇性）：誤差分析　
對 Recall@3 為 False 的樣本，分析：

* 是否檢索到語義相近但非正確規則；
* 是否因問題過短、缺乏關鍵詞導致向量匹配失敗；
* 是否因 snippet 內容模糊、表述差異造成

這能幫你識別：
> 🔸 嵌入模型語義範圍不足
> 🔸 Query 與文本語意對齊度低
> 🔸 文本表示過於稀釋等問題

---

##  最終成果與評估目標

經過這個流程，你可以量化以下問題：

| 研究問題                 | 指標             | 是否可回答 |
| -------------------- | -------------- | ----- |
| 檢索器能否找到正確規則？         | Recall@k / MRR | ✅     |
| 不同嵌入模型在規則匹配上的表現差異？   | Recall@k / MRR | ✅     |
| 是否需要在查詢中加入 scenario？ | Recall 變化      | ✅     |
| Chunk 切割策略是否影響結果？    | ❌（不適用）         |       |
| 模型能否進行邏輯推理？          | ❌（不在本實驗範圍）     |       |

